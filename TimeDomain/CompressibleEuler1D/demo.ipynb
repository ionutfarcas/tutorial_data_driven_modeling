{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressible Euler Flow of an Ideal Gas (1D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\Omega = [0,L]\\subset \\mathbb{R}$ be the spatial domain indicated by the variable $x$, and let $[t_0,t_\\text{final}]\\subset\\mathbb{R}$ be the time domain with variable $t$. We consider the one-dimensional Euler equations for the compressible flow of an ideal gas with periodic boundary conditions.\n",
    "The state is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\vec{q}_\\text{c}(x, t) = \\left[\\begin{array}{c}\n",
    "        \\rho \\\\ \\rho v \\\\ \\rho e\n",
    "    \\end{array}\\right],\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\rho = \\rho(x,t)$ is the density $[\\frac{\\text{kg}}{\\text{m}^3}]$, $v = v(x,t)$ is the fluid velocity $[\\frac{\\text{m}}{\\text{s}}]$, and $e = e(x, t)$ is the internal energy per unit mass $[\\frac{\\text{m}^2}{\\text{s}^2}]$.\n",
    "The state evolves according in time according to the following conservative system of partial differential equations (PDEs):\n",
    "\n",
    "$$\n",
    "\\tag{1.1}\n",
    "\\begin{aligned}\n",
    "    \\frac{\\partial\\vec{q}_\\text{c}}{\\partial t}\n",
    "    = \\frac{\\partial}{\\partial t} \\left[\\begin{array}{c}\n",
    "        \\rho \\\\ \\rho v \\\\ \\rho e\n",
    "    \\end{array}\\right]\n",
    "    &= -\\frac{\\partial}{\\partial x}\\left[\\begin{array}{c}\n",
    "        \\rho v \\\\ \\rho v^2 + p \\\\ (\\rho e + p) v\n",
    "    \\end{array}\\right]\n",
    "    & x &\\in\\Omega,\\quad t\\in[t_0,t_\\text{final}],\n",
    "    \\\\\n",
    "    \\vec{q}_\\text{c}(0,t) &= \\vec{q}_\\text{c}(L,t)\n",
    "    & t &\\in[t_0,t_\\text{final}],\n",
    "    \\\\\n",
    "    \\vec{q}_\\text{c}(x,t_0) &= \\vec{q}_{\\text{c},0}(x)\n",
    "    & x &\\in \\Omega,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $p = p(x,t)$ is the pressure $[\\text{Pa}] = [\\frac{\\text{kg}}{\\text{m}\\cdot\\text{s}^2}]$ and $\\vec{q}_{\\text{c},0}(x)$ is a given initial condition.\n",
    "The state variables are related via the ideal gas law\n",
    "\n",
    "$$\n",
    "\\tag{1.2}\n",
    "\\begin{aligned}\n",
    "    \\rho e = \\frac{p}{\\gamma - 1} + \\frac{1}{2}\\rho v^{2},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\gamma = 1.4$ is the nondimensional heat capacity ratio.\n",
    "\n",
    "**Important**: Note that the dynamics are nonpolynomially nonlinear with respect to $\\rho,$ $\\rho v,$ and $\\rho e$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is to construct a reduced-order model (ROM) which can be solved rapidly to produce approximate solutions to the partial differential equation $(1.1)$ for various choices of the initial condition $\\vec{q}_{\\text{c},0}$.\n",
    "We will only use data observed over a limited time interval $[t_0, t_\\text{obs}]$ with $t_\\text{obs} < t_\\text{final}$ to train the ROM, but the ROM will be used to predict the solution for the entire time domain $[t_0, t_\\text{final}]$.\n",
    "Hence, the ROM will be **predictive in time** and **predictive in the initial conditions**.\n",
    "\n",
    "We make use of the following standard scientific python libraries.\n",
    "See [demo-with-opinf-package.ipynb](./demo-with-opinf-package.ipynb) for a version of this notebook that uses the [`opinf`](https://willcox-research-group.github.io/rom-operator-inference-Python3/source/index.html) package for model reduction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import scipy.integrate\n",
    "import scipy.linalg as la\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generation and visualization routines are defined in the auxiliary file [utils.py](./utils.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "utils.configure_matplotlib(latex_is_installed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Acquire Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate data, we first define an equidistant grid $\\{x_i\\}_{i=0}^{n_x}$ over the one-dimensional spatial domain $\\Omega$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    0 &= x_0 < x_1 < \\cdots < x_{n_x-1} < x_{n_x} = L,\n",
    "    &\n",
    "    \\delta x &= \\frac{L}{n_x} = x_{i+1} - x_{i},\\quad i=0,\\ldots,n_x-1.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The spatially discretized state vector collects the values of the three state variables at each point in the spatial domain. Because periodic boundary conditions prescribe $q_\\text{c}(x_0,t) = q_\\text{c}(x_{n_x},t)$, values at the endpoint $x = x_{n_x} = L$ are not included.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\mathbf{q}_\\text{c}(t)\n",
    "    = \\left[\\begin{array}{c}\n",
    "        \\\\\n",
    "        \\boldsymbol{\\rho}(t)\n",
    "        \\\\ \\\\ \\hline \\\\\n",
    "        \\boldsymbol{\\rho}\\mathbf{v}(t)\n",
    "        \\\\ \\\\ \\hline \\\\\n",
    "        \\boldsymbol{\\rho}\\mathbf{e}(t)\n",
    "        \\\\ \\phantom{.}\n",
    "    \\end{array}\\right]\n",
    "    = \\left[\\begin{array}{c}\n",
    "        \\\\\n",
    "        \\rho(x_{0}, t) \\\\ \\vdots \\\\ \\rho(x_{n_{x}-1}, t)\n",
    "        \\\\ \\\\ \\hline \\\\\n",
    "        (\\rho v)(x_{0}, t) \\\\ \\vdots \\\\ (\\rho v)(x_{n_{x}-1}, t)\n",
    "        \\\\ \\\\ \\hline \\\\\n",
    "        (\\rho e)(x_{0}, t) \\\\ \\vdots \\\\ (\\rho e)(x_{n_{x}-1}, t)\n",
    "        \\\\ \\phantom{.}\n",
    "    \\end{array}\\right]\n",
    "    \\in\\mathbb{R}^{3n_x}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Our overall goal is to approximate $\\mathbf{q}_\\text{c}(t)$ over a time grid, which we take to be an equidistant grid of $n_t$ instances starting at $t = 0$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    0 &= t_0 < t_1 < \\cdots < t_{n_t - 1} = t_\\text{final},\n",
    "    &\n",
    "    \\delta t &= \\frac{t_\\text{final}}{n_t - 1} = t_{j+1} - t_{j}, \\quad j=0,\\ldots,n_t-2.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full-order Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A full-order model (FOM) for the PDE $(1.1)$ is a system of $n = 3n_x$ ODEs that defines evolution equations for the spatially discretized state $\\mathbf{q}_{c}(t)$.\n",
    "The class [`utils.EulerFiniteDifferenceModel`](./utils.py) implements a FOM by approximating the spatial derivative $\\frac{\\partial}{\\partial x}$ with a first-order backward finite difference approximation,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\frac{\\partial}{\\partial x}y(x,t)\n",
    "    \\approx \\frac{y(x, t) - y(x - \\delta x, t)}{\\delta x},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for $y = \\rho v,$ $\\rho v^2 + p,$ and $(\\rho e + p) v.$\n",
    "The resulting system is integrated in time using an adaptive fourth/fifth order Runge–Kutta method.\n",
    "\n",
    "For this experiment, training states are generated for several different initial conditions, determined by fixing the pressure at $10^5~[\\text{Pa}]$ and constructing splines for the density and the velocity.\n",
    "\n",
    "The next code block loads the experiment data.\n",
    "Note carefully that we **only load data** – the outputs of the FOM – as opposed to loading the FOM itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = utils.load_experiment_data()\n",
    "\n",
    "gamma = data[\"gamma\"]  # Heat capacity ratio.\n",
    "\n",
    "x = data[\"x\"]  # Spatial domain\n",
    "dx = x[1] - x[0]  # Spatial step size.\n",
    "\n",
    "t_all = data[\"t_all\"]  # Full time domain of interest.\n",
    "dt = t_all[1] - t_all[0]  # Time step size.\n",
    "\n",
    "t_train = data[\"t_train\"]  # Shorter time domain for training data.\n",
    "Q_fom = data[\"training_snapshots\"]  # Training states, defined over `t_train`.\n",
    "\n",
    "print(\n",
    "    f\"\\nSpatial domain:\\t\\t[{x[0]}, {x[-1]}] with {x.size} spatial points\",\n",
    "    f\"Spatial step size:\\t{dx=:.3f}\",\n",
    "    f\"\\nFull time domain:\\t[{t_all[0]}, {t_all[-1]}] \"\n",
    "    f\"with {t_all.size} time instances\",\n",
    "    f\"Training time domain:\\t[{t_train[0]}, {t_train[-1]}] \"\n",
    "    f\"with {t_train.size} time instances\",\n",
    "    f\"Time step size:\\t\\t{dt=:.5f}\",\n",
    "    f\"\\n{Q_fom.shape[0]} training trajectories \"\n",
    "    f\"of {Q_fom.shape[-1]} snapshots each\",\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check: Visualize Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before learning a model from data, it is always a good idea to visualize the data and check that it makes sense physically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_initial_conditions(x, Q_fom)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = utils.plot_traces(x, t_train, Q_fom[0])\n",
    "axes[0].set_title(\"Training trajectory with initial condition 0\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pre-Process Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this problem requires some pre-processing before applying a dimensionality reduction technique. However, not every problem requires these steps, and the type of preprocessing that is most appropriate for another problem may be different than what is presented here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2A: Lift to a Polynomial Form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operator inference (OpInf) uses training snapshots to construct ROMs with polynomial structure. The FOM for $(1.1)$ is not polynomial with respect to the state $\\mathbf{q}_\\text{c}(t)$, but by changing to the variables $\\vec{q} = (v, p, \\zeta)$, where $\\zeta = 1/\\rho$ is the specific volume $[\\text{m}^3/\\text{kg}]$, the PDE $(1.1)$ can be expressed equivalently as\n",
    "\n",
    "$$\n",
    "\\tag{1.3}\n",
    "\\begin{aligned}\n",
    "    \\frac{\\partial\\vec{q}}{\\partial t}\n",
    "    = \\frac{\\partial}{\\partial t} \\left[\\begin{array}{c}\n",
    "        v \\\\ \\\\ p \\\\ \\\\ \\zeta\n",
    "    \\end{array}\\right]\n",
    "    &= \\left[\\begin{array}{c}\n",
    "        -v \\frac{\\partial v}{\\partial x} - \\zeta\\frac{\\partial p}{\\partial x}\n",
    "        \\\\ \\\\\n",
    "        -\\gamma p \\frac{\\partial v}{\\partial x} - v\\frac{\\partial p}{\\partial x}\n",
    "        \\\\ \\\\\n",
    "        -v \\frac{\\partial\\zeta}{\\partial x} + \\zeta\\frac{\\partial v}{\\partial x}\n",
    "    \\end{array}\\right],\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which is quadratic with respect to $\\vec{q}$.\n",
    "This will motivate learning a ROM with a quadratic structure.\n",
    "\n",
    "The next code block defines the transformation from the original variables to the new variables, as well as the inverse transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lift(original_state):\n",
    "    \"\"\"Map the conservative variables to the specific volume variables,\n",
    "    [rho, rho*v, rho*e] -> [v, p, 1/rho].\n",
    "    \"\"\"\n",
    "    rho, rho_v, rho_e = np.split(original_state, 3, axis=0)\n",
    "\n",
    "    v = rho_v / rho\n",
    "    p = (gamma - 1) * (rho_e - 0.5 * rho_v * v)  # From the ideal gas law.\n",
    "    zeta = 1 / rho\n",
    "\n",
    "    return np.concatenate((v, p, zeta))\n",
    "\n",
    "\n",
    "def unlift(lifted_state):\n",
    "    \"\"\"Map the specific volume variables to the conservative variables,\n",
    "    [v, p, 1/rho] -> [rho, rho*v, rho*e].\n",
    "    \"\"\"\n",
    "    v, p, zeta = np.split(lifted_state, 3, axis=0)\n",
    "\n",
    "    rho = 1 / zeta\n",
    "    rho_v = rho * v\n",
    "    rho_e = p / (gamma - 1) + 0.5 * rho_v * v  # From the ideal gas law.\n",
    "\n",
    "    return np.concatenate((rho, rho_v, rho_e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the variable transformation to the training snapshots.\n",
    "Recall that `Q_fom` contains several training trajectories, each of which corresponds to different initial conditions.\n",
    "It is be helpful for later on to keep track of the trajectories separately instead of concatenating them into a single snapshot matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_fom_lifted = np.array([lift(Q) for Q in Q_fom])\n",
    "Q_fom_lifted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: In this example, we start with three state variables and apply a variable transformation to get three different state variables. However, this step is often called \"lifting\" because it is possible to introduce more variables than we started with through the transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2B: Center and/or Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables $\\vec{q} = (v, p, \\zeta)$ have significantly different characteristic scales, which makes it difficult to equally represent all three variables with a single low-dimensional approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_ranges(snapshots):\n",
    "    for variable, name in zip(\n",
    "        np.split(np.hstack(snapshots), 3),\n",
    "        (\"velocity\", \"pressure\", \"spec.vol\"),\n",
    "    ):\n",
    "        print(f\"Range({name}) = [{variable.min():.2e}, {variable.max():.2e}]\")\n",
    "\n",
    "\n",
    "data_ranges(Q_fom_lifted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address this issue, we non-dimensionalize (rescale) the training data so that each variable has the same characteristic scale.\n",
    "In this example, we select characteristic scales $\\bar{v} = 100~[\\text{m}/\\text{s}]$ for the velocity and $\\bar{p} = 10^5~[\\text{Pa}=\\text{kg}/\\text{ms}^2]$ for the pressure.\n",
    "A corresponding characteristic scale for specific volume can then be determined from the ideal gas law $(1.2)$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\bar{\\zeta}\n",
    "    = \\frac{\\bar{v}^2}{\\bar{p}}\n",
    "    = \\frac{(100)^2~[\\text{m}^{2}/\\text{s}^{2}]}{10^5~[\\text{kg}/\\text{ms}^2]}\n",
    "    = \\frac{1}{10}~[\\text{m}^3/\\text{kg}].\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "New, scaled variables are then defined as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    v' = v / \\bar{v},\n",
    "    \\qquad\n",
    "    p' = p / \\bar{p},\n",
    "    \\qquad\n",
    "    \\zeta' = \\zeta / \\bar{\\zeta}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characteristic scales for each variable.\n",
    "v_bar = 1e2\n",
    "p_bar = 1e5\n",
    "z_bar = 1e-1  # = v_bar**2 / p_bar\n",
    "\n",
    "\n",
    "def scale(snapshots):\n",
    "    \"\"\"Nondimensionalize the states [v, p, zeta] -> [v', p', zeta'].\"\"\"\n",
    "    v, p, zeta = np.split(snapshots, 3, axis=0)\n",
    "    return np.concatenate([v / v_bar, p / p_bar, zeta / z_bar], axis=0)\n",
    "\n",
    "\n",
    "def unscale(scaled_snapshots):\n",
    "    \"\"\"Redimensionalize the states [v', p', zeta'] -> [v, p, zeta].\"\"\"\n",
    "    v, p, zeta = np.split(scaled_snapshots, 3, axis=0)\n",
    "    return np.concatenate([v * v_bar, p * p_bar, zeta * z_bar], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale the variables in each training trajectory.\n",
    "Q_fom_scaled = np.array([scale(Q) for Q in Q_fom_lifted])\n",
    "\n",
    "data_ranges(Q_fom_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: There are many ways to preprocess data appropriately for the purpose of learning reduced models.\n",
    "\n",
    "- A common preprocessing step is to center the data around zero before applying a scaling. Note that this can change the form of the ROM to be learned, see [demo-with-opinf-package.ipynb](./demo-with-opinf-package.ipynb) for more details.\n",
    "- The scaling applied above is based on a brief dimensional analysis of the lifted variables. In more complex settings, it may be more convenient to apply a data-driven scaling (e.g., dividing each variable by its mean value) or to target a certain range for the scaled variables, usually $[-1, 1]$ or $[0, 1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Reduce Data Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have discretized, lifted, scaled snapshot data stored in the array `Q_fom_scaled`.\n",
    "Our goal now is to approximate the preprocessed state $\\mathbf{q}(t)\\in\\mathbb{R}^{n},$ $n = 3n_x,$ with only $r$ degrees of freedom for some small $r \\ll n$.\n",
    "That is, we want a matrix $\\mathbf{V}_{\\!r}\\in\\mathbb{R}^{n \\times r}$ with orthonormal columns such that $\\mathbf{q}(t) \\approx \\mathbf{V}_{\\!r}\\hat{\\mathbf{q}}(t)$ for some $\\hat{\\mathbf{q}}(t)\\in\\mathbb{R}^{r}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute a POD Basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common choice for $\\mathbf{V}_{\\!r}$ is the proper orthogonal decomposition (POD) basis, obtained from the singular value decomposition (SVD) of the training data.\n",
    "Let $\\mathbf{Q}\\in\\mathbb{R}^{n \\times k}$ be the matrix of all preprocessed snapshots as columns.\n",
    "The SVD of $\\mathbf{Q}$ is a matrix factorization\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\mathbf{Q} = \\boldsymbol{\\Phi\\Sigma\\Psi}^\\mathsf{T},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\Phi}\\in\\mathbb{R}^{n \\times n}$ has orthonormal columns, $\\boldsymbol{\\Sigma}\\in\\mathbb{R}^{n \\times n}$ is diagonal, and $\\boldsymbol{\\Psi}^\\mathsf{T}\\in\\mathbb{R}^{n \\times k}$ has orthonormal rows.\n",
    "Then $\\mathbf{V}_{\\!r}$ is defined to be the first $r$ columns of $\\boldsymbol{\\Phi}$, i.e., the first $r$ principal left singular vectors of the matrix of preprocessed training snapshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.hstack(Q_fom_scaled)\n",
    "Phi, sigma, PsiT = la.svd(Q, full_matrices=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: In larger problems, it is common for the dimension of the preprocessed training snapshots to be much larger than the number of training snapshots, i.e., $n \\gg k$. In this case, $\\boldsymbol{\\Phi}\\in\\mathbb{R}^{n \\times k}$, $\\boldsymbol{\\Sigma}\\in\\mathbb{R}^{k \\times k}$, and $\\boldsymbol{\\Psi}^\\mathsf{T}\\in\\mathbb{R}^{k \\times k}.$ An alternative to the SVD in this scenario is to eigendecompose $\\mathbf{Q}^\\mathsf{T}\\mathbf{Q}\\in\\mathbb{R}^{k \\times k}$ to get the first $k$ singular vectors of $\\mathbf{Q}.$ This strategy is known as the _method of snapshots_ in the literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the Reduced Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to select the reduced dimension $r$ when using POD is to examine the singular values of the training snapshots, the diagonal values of $\\boldsymbol{\\Sigma}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_singular_values(sigma, upto=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we choose $r = 9$ because of the large singular value gap between $r = 9$ and $r = 10$.\n",
    "\n",
    "<!-- In classical model reduction (e.g., balanced truncation), we never truncate the modes in the middle of a plateau of singular values because the modes in a plateau all have similar physical relevance. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definte the basis matrix as dominant left singular vectors.\n",
    "r = 9\n",
    "Vr = Phi[:, :r]\n",
    "\n",
    "# Plot the columns of the basis matrix.\n",
    "utils.plot_basis_vectors(x, Vr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions of any ROM that uses the basis matrix $\\mathbf{V}_{\\!r}$ are restricted to the span of these basis vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compress Pre-Processed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a basis $\\mathbf{V}_{\\!r}\\in\\mathbb{R}^{n \\times r}$ in hand, we compress the training snapshots by finding, for each training snapshot $\\mathbf{q}_j\\in\\mathbb{R}^{n}$, the best latent coordinates $\\hat{\\mathbf{q}}_j\\in\\mathbb{R}^{r}$ minimizing the reconstruction error $\\|\\mathbf{q}_j - \\mathbf{V}_{\\!r}\\hat{\\mathbf{q}}_j\\|_2$.\n",
    "Because $\\mathbf{V}_{\\!r}$ has orthonormal columns, this turns out to be $\\hat{\\mathbf{q}}_j = \\mathbf{V}_{\\!r}^\\mathsf{T}\\mathbf{q}_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(q):\n",
    "    \"\"\"Map (scaled) high-dimensional states to low-dimensional coordinates.\"\"\"\n",
    "    return Vr.T @ q\n",
    "\n",
    "\n",
    "def decompress(qhat):\n",
    "    \"\"\"Map low-dimensional coordinates to high-dimensional (scaled) states.\"\"\"\n",
    "    return Vr @ qhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_compressed = np.array([compress(Q) for Q in Q_fom_scaled])\n",
    "\n",
    "print(f\"Scaled FOM snapshots array: {Q_fom_scaled.shape=}\")\n",
    "print(f\"Compressed snapshots array: {Q_compressed.shape=}\")\n",
    "print(f\"Dimension reduction: n={Vr.shape[0]} -> r={Vr.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check: Reconstruction Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reconstruction error of the training snapshots, defined by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\sum_{j}\\|\\mathbf{q}_j - \\mathbf{V}_{\\!r}\\hat{\\mathbf{q}}_j\\|_2\n",
    "    = \\sum_{j}\\|\\mathbf{q}_j - \\mathbf{V}_{\\!r}\\mathbf{V}_{\\!r}^\\mathsf{T}\\mathbf{q}_j\\|_2,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "gives a sense for how faithfully this basis can approximate the true state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_fom_projected = np.array([decompress(Qhat) for Qhat in Q_compressed])\n",
    "reconstruction_error = la.norm(Q_fom_scaled - Q_fom_projected)\n",
    "relative_recon_error = reconstruction_error / la.norm(Q_fom_scaled)\n",
    "\n",
    "print(f\"Relative reconstruction error: {relative_recon_error:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unexample: What if the Data are NOT Scaled?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing a basis from *unscaled* data can result in a poor low-dimensional state approximation and issues in the eventual ROM.\n",
    "The next block computes a POD basis from the lifted but unscaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a POD basis from the lifted (but unscaled) data.\n",
    "Phi2, sigma2, PsiT2 = la.svd(np.hstack(Q_fom_lifted))\n",
    "Vr_bad = Phi2[:, :r]\n",
    "\n",
    "# Visualize the resulting basis.\n",
    "utils.plot_basis_vectors(x, Vr_bad, sharey=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the drastic difference in scales across the state variables.\n",
    "Because the characteristic scale of the pressure variable is so much larger than the characteristic scale for velocity and specific volume, the basis vectors for velocity and specific volume are smothered.\n",
    "The problem is also apparent when examining the reconstruction of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress the lifted (but unscaled) data with the corresponding POD basis.\n",
    "Q_fom_lifted_all = np.hstack(Q_fom_lifted)\n",
    "Q_compressed_bad = Vr_bad.T @ Q_fom_lifted_all\n",
    "\n",
    "# Try to reconstruct the lifted data and calculate the error.\n",
    "Q_projected_bad = Vr_bad @ (Q_compressed_bad)\n",
    "error_bad_all = la.norm(Q_fom_lifted_all - Q_projected_bad)\n",
    "relative_error_bad_all = error_bad_all / la.norm(Q_fom_lifted_all)\n",
    "\n",
    "print(f\"Relative recon. error of all variables: {relative_error_bad_all:.2%}\")\n",
    "\n",
    "# Examine the reconstruction error for the individual state variables.\n",
    "for i, (full_variable, Vr_bad_var) in enumerate(\n",
    "    zip(\n",
    "        np.split(Q_fom_lifted_all, 3, axis=0),\n",
    "        np.split(Vr_bad, 3, axis=0),\n",
    "    )\n",
    "):\n",
    "    var_projected_bad = Vr_bad_var @ Q_compressed_bad\n",
    "    var_error = la.norm(full_variable - var_projected_bad)\n",
    "    relative_var_error = var_error / la.norm(full_variable)\n",
    "    print(\n",
    "        f\"Relative recon. error for variable {i+1}: {relative_var_error:.2%}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pressure is represented well, but velocity and specific volume are not.\n",
    "Plotting the reconstruction shows the extent of the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project one training trajectory with the unscaled basis.\n",
    "Q_lifted0 = Q_fom_lifted[0]\n",
    "Q_lifted0_badprojection = Vr_bad @ (Vr_bad.T @ Q_lifted0)\n",
    "\n",
    "fig, axes = utils.plot_traces(x, t_train, Q_lifted0, lifted=True)\n",
    "axes[0].set_title(\"Original lifted data\")\n",
    "\n",
    "fig, axes = utils.plot_traces(x, t_train, Q_lifted0_badprojection, lifted=True)\n",
    "axes[0].set_title(\"Reconstruction of lifted data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Learn Reduced Operators from Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have compressed state data, instances of the reduced state variable $\\hat{\\mathbf{q}}(t)\\in\\mathbb{R}^{r}$.\n",
    "The next step is to learn evolution equations for $\\hat{\\mathbf{q}}(t)$, i.e., to choose an appropriate $\\hat{\\mathbf{f}}(t, \\hat{\\mathbf{q}}(t))$ such that $\\frac{\\text{d}}{\\text{d}t}\\hat{\\mathbf{q}}(t) \\approx \\hat{\\mathbf{f}}(t, \\hat{\\mathbf{q}}(t))$.\n",
    "\n",
    "A FOM based on finite differences for the discretized, lifted, scaled state $\\mathbf{q}(t)$ would be quadratic due to the quadratic structure of the PDE $(1.3)$ governing the lifted variables,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\frac{\\text{d}}{\\text{d}t}\\mathbf{q}(t)\n",
    "    = \\mathbf{H}[\\mathbf{q}(t) \\otimes \\mathbf{q}(t)]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for some $\\mathbf{H}\\in\\mathbb{R}^{n \\times n^2}$, where $\\otimes$ denotes the [Kronecker product](https://en.wikipedia.org/wiki/Kronecker_product).\n",
    "Inserting the approximation $\\mathbf{q}(t) \\approx \\mathbf{V}_{\\!r}\\hat{\\mathbf{q}}(t)$ and using the orthogonality property $\\mathbf{V}_{\\!r}^\\mathsf{T}\\mathbf{V}_{\\!r} = \\mathbf{I}$ yields\n",
    "\n",
    "$$\n",
    "\\tag{1.4}\n",
    "\\begin{aligned}\n",
    "    \\frac{\\text{d}}{\\text{d}t}\\hat{\\mathbf{q}}(t)\n",
    "    = \\hat{\\mathbf{H}}[\\hat{\\mathbf{q}}(t) \\otimes \\hat{\\mathbf{q}}(t)]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\hat{\\mathbf{H}} = \\mathbf{V}_{\\!r}^\\mathsf{T}\\mathbf{H}[\\mathbf{V}_{\\!r}\\otimes\\mathbf{V}_{\\!r}] \\in \\mathbb{R}^{r \\times r^2}$.\n",
    "However, we cannot compute $\\hat{\\mathbf{H}}$ this way because we do not have $\\mathbf{H}$.\n",
    "\n",
    "The OpInf approach to learning $(1.4)$ is to infer a suitable $\\hat{\\mathbf{H}}$ by solving the following linear least-squares regression of the compressed training data:\n",
    "\n",
    "$$\n",
    "\\tag{1.5}\n",
    "\\begin{aligned}\n",
    "    \\min_{\\hat{\\mathbf{H}}\\in\\mathbb{R}^{r \\times r^2}}\n",
    "    \\sum_{j=1}^{K}\\left\\|\n",
    "        \\hat{\\mathbf{H}}[\\hat{\\mathbf{q}}_j\\otimes\\hat{\\mathbf{q}}_j]\n",
    "        - \\dot{\\hat{\\mathbf{q}}}_j\n",
    "    \\right\\|_2^2\n",
    "    + \\lambda^2\\left\\|\\hat{\\mathbf{H}}\\right\\|_F^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where the $\\hat{\\mathbf{q}}_j\\in\\mathbb{R}^{r}$ are the compressed training snapshots, $\\dot{\\hat{\\mathbf{q}}}_j\\in\\mathbb{R}^{r}$ are corresponding time derivatives of the snapshots, and $\\lambda \\ge 0$ is a regularization hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Data Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression $(1.5)$ can be written in standard linear least-squares form as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\min_{\\hat{\\mathbf{H}}\\in\\mathbb{R}^{r\\times r^2}}\n",
    "    \\left\\|\\left(\\hat{\\mathbf{Q}} \\odot \\hat{\\mathbf{Q}}\\right)^\\mathsf{T}\\hat{\\mathbf{H}}^\\mathsf{T} - \\dot{\\hat{\\mathbf{Q}}}^\\mathsf{T}\\right\\|_F^2\n",
    "    + \\lambda^2\\left\\|\\hat{\\mathbf{H}}\\right\\|_F^2,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\hat{\\mathbf{Q}}\\in\\mathbb{R}^{r\\times k}$ contains all compressed training snapshots (as columns), $\\dot{\\hat{\\mathbf{Q}}}\\in\\mathbb{R}^{r\\times k}$ contains all corresponding time derivatives, and $\\odot$ is the Kronecker product applied columnwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin setting up this problem, we estimate the time derivatives of the compressed training snapshots using finite differences.\n",
    "**Having accurate time derivatives is crucial**; poor time derivate estimates will result in a poor ROM.\n",
    "Because the original FOM was solved with a fourth-order explicit scheme (RK45), we estimate the derivatives with fourth-order forward differences:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\frac{\\text{d}}{\\text{d}t}\\hat{\\mathbf{q}}(t)\\bigg|_{t = t_j}\n",
    "    \\approx \\frac{1}{12\\delta t}(-25\\hat{\\mathbf{q}}(t_j) + 48\\hat{\\mathbf{q}}(t_{j} + \\delta t) - 36\\hat{\\mathbf{q}}(t_{j} + 2\\delta t)\n",
    "    + 16\\hat{\\mathbf{q}}(t_{j} + 3\\delta t) - 3\\hat{\\mathbf{q}}(t_{j} + 4\\delta t)).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Several snapshots in each trajectory must also be stripped off since the forward finite difference scheme does not provide estimates for the time derivative at the last four time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = t_train[1] - t_train[0]\n",
    "\n",
    "\n",
    "def fwd4(q):\n",
    "    \"\"\"Estimate time derivatives with fourth-order forward differences.\"\"\"\n",
    "    return (\n",
    "        -25 * q[:, :-4]\n",
    "        + 48 * q[:, 1:-3]\n",
    "        - 36 * q[:, 2:-2]\n",
    "        + 16 * q[:, 3:-1]\n",
    "        - 3 * q[:, 4:]\n",
    "    ) / (12 * dt)\n",
    "\n",
    "\n",
    "Qhatdot = np.hstack([fwd4(Q) for Q in Q_compressed])\n",
    "Qhat = np.hstack([Q[:, :-4] for Q in Q_compressed])\n",
    "\n",
    "print(f\"Compressed snapshots array:     {Qhat.shape=}\")\n",
    "print(f\"Compressed ddts array:       {Qhatdot.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before constructing the snapshot data matrix $\\mathbf{D} = (\\hat{\\mathbf{Q}}\\odot\\hat{\\mathbf{Q}})^\\mathsf{T}\\in\\mathbb{R}^{k\\times r^2}$, note that the Kronecker product $\\hat{\\mathbf{q}}(t)\\otimes\\hat{\\mathbf{q}}(t)$ has redundant entries (e.g., $\\hat{q}_1\\hat{q}_2$ and $\\hat{q}_2\\hat{q}_1$).\n",
    "As a consequence, $\\mathbf{D}$ does not have full column rank and the least-squares problem $(1.5)$ does not have a unique solution when we use the full Kronecker product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the full Kronecker product in the data matrix.\n",
    "D_fullkronecker = la.khatri_rao(Qhat, Qhat).T\n",
    "D_rank = np.linalg.matrix_rank(D_fullkronecker)\n",
    "\n",
    "print(f\"D.shape = {D_fullkronecker.shape} but rank(D) = {D_rank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remedy the situation, we substitute the full Kronecker product with a _compressed Kronecker product_ that only computes the unique entries of the product.\n",
    "The compressed product has $r(r+1)/2$ entries instead of $r^2$ entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compressed_kronecker(state):\n",
    "    \"\"\"Unique entries of the Kronecker product of a vector with itself.\"\"\"\n",
    "    return np.concatenate(\n",
    "        [state[i] * state[: i + 1] for i in range(state.shape[0])],\n",
    "        axis=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can form a data matrix $\\mathbf{D} = (\\hat{\\mathbf{Q}}\\odot\\hat{\\mathbf{Q}})^\\mathsf{T}\\in\\mathbb{R}^{k\\times r(r+1)/2}$ with full column rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = compressed_kronecker(Qhat).T\n",
    "\n",
    "print(f\"{D.shape=}, rank(D)={np.linalg.matrix_rank(D)}\")\n",
    "print(f\"{Qhatdot.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data can also improve the condition number of the data matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve the Least-squares Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to solve $(1.5)$ is to solve the corresponding system of normal equations for $\\hat{\\mathbf{H}}$,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\left(\\mathbf{D}^\\mathsf{T}\\mathbf{D} + \\lambda^2\\mathbf{I}\\right)\n",
    "    \\hat{\\mathbf{H}}^{\\mathsf{T}}\n",
    "    = \\mathbf{D}^\\mathsf{T}\\dot{\\hat{\\mathbf{Q}}}^\\mathsf{T}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "There are other approaches that are more numerically stable, but this way is computationally efficient when tuning the regularization hyperparameter $\\lambda$ (see **other example**).\n",
    "For now, we fix $\\lambda = 10^{-8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form the regularization matrix.\n",
    "regularization = 1e-8\n",
    "Id = np.eye(D.shape[1])\n",
    "Reg = regularization**2 * Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form and solve the normal equations.\n",
    "Hhat = la.solve(D.T @ D + Reg, D.T @ Qhatdot.T, assume_a=\"pos\").T\n",
    "\n",
    "# Alternatively, use a least-squares solver (QR, SVD, etc.).\n",
    "# In this approach, the data matrix and the regularizer must be stacked.\n",
    "# Hhat2 = la.lstsq(\n",
    "#     np.vstack([D, Reg]),\n",
    "#     np.vstack([Qhatdot.T, np.zeros(D.shape[1], Qhatdot.shape[0]))]),\n",
    "# )[0].T\n",
    "\n",
    "print(f\"{Hhat.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the ROM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting ROM is the low-dimensional system of ODEs $(1.4)$ with the $\\hat{\\mathbf{H}}$ learned through OpInf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rom_derivative(tt, reduced_state):\n",
    "    \"\"\"Right-hand side of the reduced-order model equations.\"\"\"\n",
    "    return Hhat @ compressed_kronecker(reduced_state)\n",
    "\n",
    "\n",
    "def reduced_order_solve(qhat_init, time_domain):\n",
    "    \"\"\"Integrate the reduced-order model in time with SciPy.\"\"\"\n",
    "    return scipy.integrate.solve_ivp(\n",
    "        fun=_rom_derivative,\n",
    "        t_span=[time_domain[0], time_domain[-1]],\n",
    "        y0=qhat_init,\n",
    "        method=\"RK45\",\n",
    "        t_eval=time_domain,\n",
    "        rtol=1e-6,\n",
    "        atol=1e-9,\n",
    "    ).y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Solve the ROM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check: Reproduce Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the ROM for prediction, we use the training initial conditions and compare the ROM solutions to the compressed training data.\n",
    "This is an important sanity check: if the ROM cannot reproduce the training data, it is unlikely to be able to issue accurate predictions beyond training scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Relative training errors in the reduced space\")\n",
    "\n",
    "_start = time.time()\n",
    "for i, Qhat in enumerate(Q_compressed):\n",
    "    qhat0 = Qhat[:, 0]\n",
    "    Qhat_rom = reduced_order_solve(qhat0, t_train)\n",
    "    rom_error = la.norm(Qhat - Qhat_rom) / la.norm(Qhat)\n",
    "    print(f\"  Trajectory {i: >2d}: {rom_error:.4%}\")\n",
    "rom_solve_time = time.time() - _start\n",
    "\n",
    "print(f\"{len(Q_compressed)} ROM solves in {rom_solve_time:.6} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_reduced_trajectories(t_train, [Qhat, Qhat_rom])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Above, errors are computed with respect to the Frobenius matrix norm, which is what `scipy.linalg.norm()` computes by default.\n",
    "This is fine for this demonstration, but there are other norms that are more mathematically appropriate for describing distances between time-dependent functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue Predictions for New Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we take a new initial condition which was **not** used for training, solve the ROM, and compare it with the FOM solution.\n",
    "We need to remember to do (then undo) all preprocessing steps:\n",
    "- The new initial condition must be lifted, scaled, and compressed before it is fed to the ROM.\n",
    "- ROM solutions must be decompressed, unscaled, and unlifted before comparing with the FOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(original_state):\n",
    "    \"\"\"Map original states to the reduced coordinate space.\"\"\"\n",
    "    return compress(scale(lift(original_state)))\n",
    "\n",
    "\n",
    "def postprocess(reduced_state):\n",
    "    \"\"\"Map reduced coordinates to the original state space.\"\"\"\n",
    "    return unlift(unscale(decompress(reduced_state)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the initial condition to the reduced state space.\n",
    "_start = time.time()\n",
    "rom_init = preprocess(data[\"test_init\"])\n",
    "\n",
    "# Solve the ROM in the reduced state space.\n",
    "_start2 = time.time()\n",
    "rom_solution_reduced = reduced_order_solve(rom_init, t_all)\n",
    "rom_integration_time = time.time() - _start2\n",
    "\n",
    "# Map the ROM solution back to the original state space.\n",
    "rom_solution = postprocess(rom_solution_reduced)\n",
    "rom_solve_time = time.time() - _start\n",
    "\n",
    "print(f\"ROM integrated in {rom_integration_time:.6f} s\")\n",
    "print(f\"ROM solve done in {rom_solve_time:.6f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we test the accuracy of the ROM on this new initial condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fom_solution = data[\"test_solution\"]\n",
    "\n",
    "# Check the overall ROM error.\n",
    "test_error = la.norm(fom_solution - rom_solution) / la.norm(fom_solution)\n",
    "print(f\"Overall relative ROM test error: {test_error:.2%}\")\n",
    "\n",
    "# Check the ROM error for each of the physical variables.\n",
    "for i, (rom_var, fom_var) in enumerate(\n",
    "    zip(\n",
    "        np.split(rom_solution, 3, axis=0),\n",
    "        np.split(fom_solution, 3, axis=0),\n",
    "    )\n",
    "):\n",
    "    test_var_error = la.norm(fom_var - rom_var) / la.norm(fom_var)\n",
    "    print(f\"ROM test error of variable {i+1}: {test_var_error:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_regimes(x, t_train, t_all, fom_solution, title=\"FOM\")\n",
    "utils.plot_regimes(x, t_train, t_all, rom_solution, title=\"ROM\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [demo-with-opinf-package.ipynb](./demo-with-opinf-package.ipynb) for a version of this notebook that uses the [`opinf`](https://willcox-research-group.github.io/rom-operator-inference-Python3/source/index.html) package and performs a few additional experiments with this problem."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "cse-mt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
