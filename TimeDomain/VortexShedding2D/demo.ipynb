{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64e1f1d",
   "metadata": {},
   "source": [
    "# Vortex-shedding Flow Past a Cylinder (2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733f358a",
   "metadata": {},
   "source": [
    "We consider the canonical problem of two-dimensional transient flow past a circular cylinder, a widely-used benchmark in computational fluid dynamics and reduced-order modeling the fluid flow is governed by the 2D incompressible Navier-Stokes equations\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\partial_t \\mathbf{u} + \\nabla \\cdot (\\mathbf{u} \\otimes \\mathbf{u})\n",
    "    &= \\nabla p + Re^{-1}\\Delta \\mathbf{u}\n",
    "    \\\\\n",
    "    \\nabla \\cdot \\mathbf{u}\n",
    "    &= 0,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $p \\in \\mathbb{R}$ denotes the pressure, $\\mathbf{u} = (u_x, u_y)^\\mathsf{T} \\in \\mathbb{R}^2$ denotes the $x$ and $y$ components of the velocity vector, and $Re$ denotes the dimensionless Reynolds number.\n",
    "\n",
    "The problem setup, geometry, and parameterization are based on the [DFG 2D-3 benchmark in the FeatFlow suite](https://wwwold.mathematik.tu-dortmund.de/~featflow/en/benchmarks/cfdbenchmarking/flow/dfg_benchmark2_re100.html). This setup uses $Re = 100$, which represents a value that is above the critical Reynolds number for the onset of the two-dimensional vortex shedding the physical domain is depicted in the figure below:\n",
    "<img src=\"./navier_stokes_cylinder_geometry.png\" width=\"700\" class=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98630cec",
   "metadata": {},
   "source": [
    "We make use of the following standard scientific python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df08a1-9471-4ffc-a092-fb7f0d6bca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import itertools\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d004ce0d-543f-42db-ab50-1162dc2bfd88",
   "metadata": {},
   "source": [
    "## Step 1: Acquire Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601a7ac4",
   "metadata": {},
   "source": [
    "We solve the 2D Navier Stokes equations numerically in double precision using finite elements with the [FEniCS](https://fenicsproject.org/) software package. Our implementation closely follows the Navier-Stokes cylinder example that can be found [here](https://a654cc05c43271a5d22f-f8befe5e0dcd44ae0dccf352c00b4664.ssl.cf5.rackcdn.com/tutorial/python/vol1/).\n",
    "\n",
    "We use piecewise quadratic finite elements for the velocity and piecewise linear elements for the pressure. The resulting system of equations is large and sparse, making direct solvers impractical. We employ iterative Krylov subspace methods with preconditioning for improved performance. We utilize the biconjugate gradient stabilized (BiCGstab) method and the conjugate gradient (CG) method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6660b5-c886-4b4c-93a5-a460a9fb9863",
   "metadata": {},
   "source": [
    "### Full-order Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba2c732",
   "metadata": {},
   "source": [
    "The finite element mesh contains $n_x = 9,477$ degrees of freedom.The discretized governing equations are integrated over the time interval $[0, 10]$ seconds in increments of $\\Delta t = 10^{-3}$ seconds. This amounts to a total of $10,000$ time instants of the pressure and velocity fields. In the model reduction experiments that follow, we simplify the model by omitting the pressure term, which is justified by the fact that, for both transient and periodic flow regimes, the pressure can be implicitly determined from the velocity field through the incompressibility constraint. This means that the number of state variables is $n_s = 2$. Therefore, the size of one data snapshot is $n = n_s \\times n_x = 18,954$.\n",
    "\n",
    "The target time horizon is $[4, 10]$ seconds, corresponding to the periodic regime. Training data are collected over $[4, 7]$ seconds, whereas the remainder of the target time horizon is used for predictions beyond training. The keep the size of the training dataset reasonable, we downsample it by a factor of $10$, reducing its storage footprint to around $42$ MB for $n_t = 300$ downsampled snapshots. The training data were saved to disk in a single HDF5 file [**velocity_training_snapshots.h5**](./navier_stokes_benchmark/velocity_training_snapshots.h5). This file contains two datasets, $u\\_x$ and $u\\_y$, for the two downsampled velocity components of dimension $9,477 \\times 300$. This amounts to a snapshot matrix $\\mathbf{S} \\in \\mathbb{R}^{n \\times n_t} = \\mathbb{R}^{18,954 \\times 300}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c01fb3-05d7-44f9-a3de-482a3ffd39ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DoF setup\n",
    "ns = 2\n",
    "n = 18954\n",
    "nx = int(n / ns)\n",
    "\n",
    "nt = 300  # Number of training snapshots.\n",
    "\n",
    "# state variable names\n",
    "state_variables = [\"u_x\", \"u_y\"]\n",
    "\n",
    "# path to the HDF5 file containing the training snapshots.\n",
    "H5_training_snapshots = \"./velocity_training_snapshots.h5\"\n",
    "\n",
    "# Number of time instances over the time domain of interest\n",
    "# (including training and prediction).\n",
    "nt_p = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacd2b71-5db5-4985-a928-f1b6cb4ce632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate memory for the full snapshot data.\n",
    "# the full snapshot data has been saved to disk in HDF5 format\n",
    "Q_global = np.zeros((n, nt))\n",
    "with h5.File(H5_training_snapshots, \"r\") as hf:\n",
    "    for j in range(ns):\n",
    "        Q_global[j * nx : (j + 1) * nx, :] = hf[state_variables[j]][:]\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea6550-f113-4f1f-b930-ff21f1bd913b",
   "metadata": {},
   "source": [
    "## Step 2: Pre-Process Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018f01a2",
   "metadata": {},
   "source": [
    "In the next step, we perform any necessary data manipulations, which can vary depending on the specific problem. This might include variable transformations for scenarios with non-polynomial governing equations or scaling transformations for those involving multiple states with varying scales.\n",
    "\n",
    "If data transformations are used, we denote by $\\mathbf{Q} \\in \\mathbb{R}^{m \\times n_t}$ the *transformed* snapshot data on each compute core, with $m \\geq n$ denoting the transformed snapshot dimension. Note that $m$ exceeds $n$ when the number of lifted variables exceeds the number of original state variables. If data transformations are not required, we employ $\\mathbf{S}$ as is and use the notation $\\mathbf{Q} = \\mathbf{S}$ and $m = n$ for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a0b92-cc8f-4d9d-9bbe-002485e0c29f",
   "metadata": {},
   "source": [
    "### Data Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73813880",
   "metadata": {},
   "source": [
    "For the considered 2D Navier-Stokes example, the only employed data transformation is snapshot centering with respect to the temporal mean over the training time horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4031a374-d8a1-481f-8555-bcc3e344b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the global temporal mean of each variable over the training horizon.\n",
    "temporal_mean_global = np.mean(Q_global, axis=1)\n",
    "\n",
    "# Center (in place) each variable with respect to its global temporal mean.\n",
    "Q_global -= temporal_mean_global[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ebd198-c51b-41fc-bedf-c50b63c778f1",
   "metadata": {},
   "source": [
    "## Step 3: Reduce Data Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a65440",
   "metadata": {},
   "source": [
    "In the next step, we perform parallel dimensionality reduction, which represent the high-dimensional (transformed) snapshot data with global dimension $m$ in a lower-dimensional space of dimension $r$ such that $r \\ll m$. Here, the reduced space is a linear subspace, spanned by the column vectors forming the $r$-dimensional POD basis. This step is typically the most computationally and memory-intensive part of the standard, serial OpInf formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7af996-cd6f-4af5-8903-84454edc29b1",
   "metadata": {},
   "source": [
    "### Leveraging the method of snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b67849",
   "metadata": {},
   "source": [
    "Starting from the method of snapshots, we can efficiently represent the high-dimensional snapshot data in the low-dimensional subspace spanned by the $r$-dimensional POD basis vectors without explicitly having to compute the POD basis and without introducing approximations by using, for example, a randomized SVD technique. We start by computing the Gram matrix $\\mathbf{D} = \\mathbf{Q}^\\top \\mathbf{Q} \\in \\mathbb{R}^{n_t \\times n_t}$.\n",
    "\n",
    "Consider the thin singular value decomposition (SVD) of $\\mathbf{Q}$ computed at a cost in $\\mathcal{O}(mn_t^2)$:\n",
    "$$ \\begin{equation}\n",
    "    \\mathbf{Q} = \\mathbf{V} \\mathbf{\\Sigma} \\mathbf{W}^\\top,\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\mathbf{V} \\in \\mathbb{R}^{m \\times n_t}$ contains the left singular vectors, $\\mathbf{\\Sigma} \\in \\mathbb{R}^{n_t \\times n_t}$ is a diagonal matrix comprising the singular values in non-decreasing order $\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_{n_t}$, where $\\sigma_j$ denotes the $j$th singular value, and $\\mathbf{W} \\in \\mathbb{R}^{n_t \\times n_t}$ contains the right singular vectors. The rank-$r$ POD basis $\\mathbf{V}_r \\in \\mathbb{R}^{m \\times r}$ is obtained from the first $r$ columns of $\\mathbf{V}$ corresponding to the $r$ largest singular values.\n",
    "\n",
    "In the standard OpInf formulation, the low-dimensional representation of $\\mathbf{Q}$ in the linear subspace spanned by the columns of $\\mathbf{V}_r$ is computed as\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{Q}} = \\mathbf{V}_r^\\top \\mathbf{Q} \\in \\mathbb{R}^{r \\times n_t}\n",
    "\\end{equation}\n",
    "$$\n",
    "However, we have that\n",
    "$$\n",
    "\\begin{equation} \n",
    "    \\mathbf{D} = \\mathbf{Q}^\\top \\mathbf{Q} = \\mathbf{W} \\mathbf{\\Sigma} \\mathbf{V}^\\top \\mathbf{V} \\mathbf{\\Sigma} \\mathbf{W}^\\top = \\mathbf{W} \\mathbf{\\Sigma}^2 \\mathbf{W}^\\top \\Rightarrow \\mathbf{D} \\mathbf{W} = \\mathbf{W} \\mathbf{\\Sigma}^2,\n",
    "\\end{equation}\n",
    "$$\n",
    "which implies that the eigenvalues of $\\mathbf{D}$ are the squared singular values of $\\mathbf{Q}$, and the eigenvectors of $\\mathbf{D}$ are equivalent (up to a sign change) to the right singular vectors of $\\mathbf{Q}$. We then compute the eigenpairs $\\{(\\lambda_k, \\mathbf{u}_k)\\}_{k=1}^{n_t}$ of $\\mathbf{D}$, where $\\lambda_k$ are the real and non-negative eigenvalues and $\\mathbf{u}_k \\in \\mathbb{R}^{n_t}$ denote the corresponding eigenvectors. We also ensure that the eigenpairs are arranged such that $\\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_{n_t}$\n",
    "The rank-$r$ POD basis can be also computed as\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathbf{V}_r = \\mathbf{Q} \\mathbf{W}_r \\mathbf{\\Sigma}_r^{-1} = \\mathbf{Q} \\mathbf{U}_r \\mathbf{\\Lambda}_r^{-\\frac{1}{2}}, \n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\mathbf{U}_r = \\begin{bmatrix} \\mathbf{u}_1 \\vert \\mathbf{u}_2 \\vert \\dots \\vert \\mathbf{u}_r \\end{bmatrix}$ and $\\mathbf{\\Lambda}_r = \\mathrm{diag}(\\lambda_1, \\lambda_2, \\ldots, \\lambda_r)$.\n",
    "The above expression implies that\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\hat{\\mathbf{Q}} = \\mathbf{V}_r^\\top \\mathbf{Q} = \\left(\\mathbf{Q} \\mathbf{U}_r \\mathbf{\\Lambda}_r^{-\\frac{1}{2}}\\right)^\\top \\mathbf{Q} = \\mathbf{T}_r^\\top \\mathbf{Q}^\\top \\mathbf{Q} = \\mathbf{T}_r^\\top \\mathbf{D},\n",
    "\\end{equation}\n",
    "$$\n",
    "where we used the notation $\\mathbf{T}_r =  \\mathbf{U}_r \\mathbf{\\Lambda}_r^{-\\frac{1}{2}} \\in \\mathbb{R}^{n_t \\times r}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698af16b-7975-4c50-88c0-e2171dd5ca8b",
   "metadata": {},
   "source": [
    "### Efficient data dimensionality reduction without explicitly requiring the POD basis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc91226",
   "metadata": {},
   "source": [
    "Therefore, the representation of the high-dimensional (transformed) snapshots in the low-dimensional linear subspace spanned by the rank-$r$ POD basis vectors can be efficiently computed in terms of two small matrices, $\\mathbf{T}_r$ and $\\mathbf{D}$, without explicitly requiring the POD basis\n",
    "In our implementation, we choose the reduced dimension, $r$, such that the total retained energy corresponding to the first $r$ POD modes is $99.9 \\%$, that is,\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\frac{\\sum_{k=1}^{r} \\sigma_k^2}{\\sum_{k=1}^{n_t} \\sigma_k^2} \\geq 0.999.\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e2d7d3-269a-462d-966e-d1aa2a67bd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the global Gram matrix and get its eigendecomposition.\n",
    "D_global = Q_global.T @ Q_global\n",
    "eigs, eigv = np.linalg.eigh(D_global)\n",
    "\n",
    "# Order the eigenpairs by increasing eigenvalue magnitude.\n",
    "sorted_indices = np.argsort(eigs)[::-1]\n",
    "eigs = eigs[sorted_indices]\n",
    "eigv = eigv[:, sorted_indices]\n",
    "\n",
    "# Amount of energy to retain in the basis.\n",
    "target_ret_energy = 0.999\n",
    "\n",
    "# Determine the minimum integer r that exceeds the retained energy threshold.\n",
    "ret_energy = np.cumsum(eigs) / np.sum(eigs)\n",
    "r = np.argmax(ret_energy > target_ret_energy) + 1\n",
    "\n",
    "# Compute the auxiliary Tr matrix.\n",
    "Tr_global = eigv[:, :r] @ np.diag(eigs[:r] ** (-0.5))\n",
    "\n",
    "# Efficiently compute the low-dimensional representation of the\n",
    "# high-dimensional transformed snapshot data.\n",
    "Qhat_global = Tr_global.T @ D_global"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7a9a68-d108-43df-8fef-5391f7740606",
   "metadata": {},
   "source": [
    "## Step 4: Learn Reduced Operators from Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507cdb7a",
   "metadata": {},
   "source": [
    "The Navier-Stokes equations have only linear and quadratic terms in the governing equations. We additionally have a constant term introduced into the ROM, due to centering. Since the training snapshots were downsampled in time, we employ the time-discrete formulation of OpInf.\n",
    "\n",
    "The goal is to determine the reduced operators $\\hat{\\mathbf{c}} \\in \\mathbb{R}^{r}, \\hat{\\mathbf{A}} \\in \\mathbb{R}^{r\\times r}$, and $\\hat{\\mathbf{H}} \\in \\mathbb{R}^{r\\times r^2}$ defining the discrete quadratic ROM\n",
    "$$\n",
    "\\begin{equation} \n",
    "    \\hat{\\mathbf{q}}[k + 1] = \\hat{\\mathbf{A}}\\hat{\\mathbf{q}}[k] + \\hat{\\mathbf{H}}\\left(\\hat{\\mathbf{q}}[k] \\otimes \\hat{\\mathbf{q}}[k] \\right) + \\hat{\\mathbf{c}}\n",
    "\\end{equation}\n",
    "$$\n",
    "that best match the projected snapshot data in a minimum residual sense by solving the linear least-squares minimization\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathop{\\mathrm{argmin}}_{\\hat{\\mathbf{O}}} \\left\\lVert \\hat{\\mathbf{D}}\\hat{\\mathbf{O}}^{\\top} - \\hat{\\mathbf{Q}}_2^\\top \\right\\rVert_F^2 + \\beta_{1} \\left(\\left\\lVert\\hat{\\mathbf{A}}\\right\\rVert_F^2 + \\left\\lVert\\hat{\\mathbf{c}}\\right\\rVert_F^2\\right) + \\beta_{2} \\left\\lVert\\hat{\\mathbf{H}}\\right\\rVert_F^2,\n",
    "\\end{equation}\n",
    "$$\n",
    "where  $\\hat{\\mathbf{O}} =\n",
    "\\begin{bmatrix}\n",
    "\\hat{\\mathbf{A}} \\, \\vert \\, \\hat{\\mathbf{H}} \\, \\vert \\, \\hat{\\mathbf{c}}\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{r \\times (r + r^2 + 1)}$ denotes the unknown operators and  $\\hat{\\mathbf{D}} =\n",
    "\\begin{bmatrix}\n",
    "\\hat{\\mathbf{Q}}_1^\\top \\, \\vert \\, \\hat{\\mathbf{Q}}_1^\\top \\otimes \\hat{\\mathbf{Q}}_1^\\top \\, \\vert \\, \\hat{\\mathbf{1}}_{n_t - 1}\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{(n_t - 1) \\times (r + r^2 +1)}$ the OpInf data, $F$ denotes the Frobenius norm, and\n",
    "\\begin{equation} \n",
    "    \\hat{\\mathbf{Q}}_{1} =\n",
    "     \\begin{bmatrix}\n",
    "\\vert & \\vert & & \\vert\\\\\n",
    "     \\hat{\\mathbf{q}}_1 &\n",
    "     \\hat{\\mathbf{q}}_2 &\n",
    "     \\ldots &\n",
    "     \\hat{\\mathbf{q}}_{n_t - 1}\\\\\n",
    "     \\vert & \\vert & & \\vert\n",
    "     \\end{bmatrix} \\in \\mathbb{R}^{r \\times n_t-1} \\quad \\text{and} \\quad \n",
    "     \\hat{\\mathbf{Q}}_{2} =\n",
    "     \\begin{bmatrix}\n",
    "\\vert & \\vert & & \\vert\\\\\n",
    "     \\hat{\\mathbf{q}}_2 &\n",
    "     \\hat{\\mathbf{q}}_3 &\n",
    "     \\ldots &\n",
    "     \\hat{\\mathbf{q}}_{n_t}\\\\\n",
    "     \\vert & \\vert & & \\vert\n",
    "     \\end{bmatrix} \\in \\mathbb{R}^{r \\times n_t-1}\n",
    "\\end{equation}\n",
    "\n",
    "To address overfitting and other sources of error, we introduce regularization hyperparameters $\\beta_{1}, \\beta_{2} \\in \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a534666d-821e-4d93-a652-cae9dc8feace",
   "metadata": {},
   "source": [
    "### Defining some useful auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52f2317",
   "metadata": {},
   "source": [
    "We start with providing the code for four auxiliary functions that we will need later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dab116-6c29-46f4-a1b9-f1b2c18c16c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alternative to compute_Qhat_sq() used in euler.ipynb:\n",
    "# def compressed_kronecker(state):\n",
    "#     \"\"\"Unique entries of the Kronecker product of a vector with itself.\"\"\"\n",
    "#     return np.concatenate(\n",
    "#         [state[i] * state[: i + 1] for i in range(state.shape[0])],\n",
    "#         axis=0,\n",
    "#     )\n",
    "\n",
    "\n",
    "def compute_Qhat_sq(Qhat):\n",
    "    r\"\"\"Compute the non-redundant terms in Qhat \\otimes Qhat.\"\"\"\n",
    "\n",
    "    if len(np.shape(Qhat)) == 1:\n",
    "        r = np.size(Qhat)\n",
    "        prods = []\n",
    "        for i in range(r):\n",
    "            temp = Qhat[i] * Qhat[i:]\n",
    "            prods.append(temp)\n",
    "\n",
    "        Qhat_sq = np.concatenate(tuple(prods))\n",
    "\n",
    "    elif len(np.shape(Qhat)) == 2:\n",
    "        K, r = np.shape(Qhat)\n",
    "        prods = []\n",
    "        for i in range(r):\n",
    "            temp = (\n",
    "                np.transpose(np.broadcast_to(Qhat[:, i], (r - i, K)))\n",
    "                * Qhat[:, i:]\n",
    "            )\n",
    "            prods.append(temp)\n",
    "\n",
    "        Qhat_sq = np.concatenate(tuple(prods), axis=1)\n",
    "\n",
    "    else:\n",
    "        print(\"invalid input!\")\n",
    "\n",
    "    return Qhat_sq\n",
    "\n",
    "\n",
    "def compute_train_err(Qhat_train, Qtilde_train):\n",
    "    \"\"\"Computes the OpInf training error.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Qhat_train : (r, k) ndarray\n",
    "        Reference data, the high-dimensional snapshots after\n",
    "        data transformation and compression.\n",
    "    Qtilde_train : (r, k) ndarray\n",
    "        Solution to the OpInf ROM that should match the reference data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_error : float\n",
    "        Error between the reference data and the ROM solution.\n",
    "    \"\"\"\n",
    "    # return np.max(\n",
    "    #     la.norm(Qtilde_train - Qhat_train, axis=1)\n",
    "    #     / la.norm(Qhat_train, axis=1)\n",
    "    # )\n",
    "    return np.max(\n",
    "        np.sqrt(\n",
    "            np.sum((Qtilde_train - Qhat_train) ** 2, axis=1)\n",
    "            / np.sum(Qhat_train**2, axis=1)\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def solve_opinf_difference_model(initial_condition, n_steps, reduced_model):\n",
    "    \"\"\"Solve the discrete OpInf ROM over a prescribed trial time horizon.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    initial_condition : (r,) ndarray\n",
    "        Initial condition in the reduced space.\n",
    "    n_steps : int\n",
    "        Number of time steps to compute the ROM solution.\n",
    "    reduced_model : callable\n",
    "        Function for the right-hand side of the ROM.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    contains_nans : bool\n",
    "        ``True`` if the solution contains ``NaN``, ``False`` otherwise.\n",
    "    Qtilde : (n_steps, r) ndarray\n",
    "        Solution to the ROM.\n",
    "    \"\"\"\n",
    "\n",
    "    Qtilde = np.zeros((np.size(initial_condition), n_steps))\n",
    "    contains_nans = False\n",
    "\n",
    "    Qtilde[:, 0] = initial_condition\n",
    "    for i in range(n_steps - 1):\n",
    "        Qtilde[:, i + 1] = reduced_model(Qtilde[:, i])\n",
    "\n",
    "    if np.any(np.isnan(Qtilde)):\n",
    "        contains_nans = True\n",
    "\n",
    "    return contains_nans, Qtilde.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4d1f8f-cf11-4187-9020-2b396b5721c8",
   "metadata": {},
   "source": [
    "### Finding the optimal hyperparameter values via a grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8faeaac",
   "metadata": {},
   "source": [
    "We employ a grid search to find the optimal hyperparameter values. This involves exploring a range of candidate values for both $\\beta_{1}$ and $\\beta_{2} \\in \\mathbb{R}$ in a nested loop. We prescribe discrete sets of candidate values $\\mathcal{B}_1$ and $\\mathcal{B}_2$ for the regularization parameters.\n",
    "\n",
    "We consider eight candidate values for both $\\mathcal{B}_1$ and $\\mathcal{B}_2$, noting that in problems with more complex dynamics, sets with larger cardinalities (and bounds) might be necessary to ensure that the optimal pair leads to accurately inferred reduced operators. We also prescribe a tolerance for the maximum growth of the inferred reduced coefficients over the trial time horizon, which will be used to determine the optimal regularization pair. The optimal hyperparameters are chosen to minimize the training error, subject to the constraint that the inferred reduced coefficient have bounded growth over a trial time horizon $[t_{\\mathrm{init}}, t_{\\mathrm{trial}}]$ with $t_{\\mathrm{trial}} \\geq t_{\\mathrm{final}}$; in our implementation, the trial time horizon is the same as the target horizon, that is, $[4, 10]$ seconds. Moreover, the solution to the OpInf least-squares minimization is determined by solving the normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f908289-f9a8-439d-841a-70f18980e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ranges for the regularization parameter pairs.\n",
    "B1 = np.logspace(-10.0, 0.0, num=8)\n",
    "B2 = np.logspace(-4.0, 4.0, num=8)\n",
    "\n",
    "# Get the Cartesian product of all regularization pairs (beta1, beta2).\n",
    "reg_pairs_global = list(itertools.product(B1, B2))\n",
    "n_reg_global = len(reg_pairs_global)\n",
    "\n",
    "# Set the threshold for the maximum growth of the inferred reduced\n",
    "# coefficients, used for selecting the optimal regularization parameter pair.\n",
    "max_growth = 1.2\n",
    "\n",
    "# Extract left and right shifted reduced data matrices for discrete OpInf.\n",
    "Qhat_1 = Qhat_global.T[:-1, :]\n",
    "Qhat_2 = Qhat_global.T[1:, :]\n",
    "\n",
    "# Column dimension of the data matrix Dhat used in discrete OpInf.\n",
    "s = int(r * (r + 1) / 2)\n",
    "d = r + s + 1\n",
    "\n",
    "# Compute the non-redundant quadratic terms of Qhat_1 squared.\n",
    "Qhat_1_sq = compute_Qhat_sq(Qhat_1)\n",
    "\n",
    "# Define the constant part (due to mean shifting) for discrete OpInf.\n",
    "K = Qhat_1.shape[0]\n",
    "Ehat = np.ones((K, 1))\n",
    "\n",
    "# Assemble the data matrix Dhat for the discrete OpInf learning problem.\n",
    "Dhat = np.concatenate((Qhat_1, Qhat_1_sq, Ehat), axis=1)\n",
    "# Compute Dhat^T Dhat for the normal equations to solve the least squares.\n",
    "Dhat_2 = Dhat.T @ Dhat\n",
    "\n",
    "# Compute the temporal mean and maximum deviation of the reduced training data.\n",
    "mean_Qhat_train = np.mean(Qhat_global.T, axis=0)\n",
    "max_diff_Qhat_train = np.max(np.abs(Qhat_global.T - mean_Qhat_train), axis=0)\n",
    "\n",
    "# Loop over all regularization pairs.\n",
    "best_train_err = 1e20\n",
    "best_beta1, best_beta2 = None, None\n",
    "for beta1, beta2 in reg_pairs_global:\n",
    "\n",
    "    start_time_OpInf_learning = time.time()\n",
    "\n",
    "    # Construct a regularizer that penalizes the linear and constant reduced\n",
    "    # operators using beta1 and the quadratic operator using beta2.\n",
    "    regg = np.zeros(d)\n",
    "    regg[:r] = beta1\n",
    "    regg[r : r + s] = beta2\n",
    "    regg[r + s :] = beta1\n",
    "    regularizer = np.diag(regg)\n",
    "    Dhat_2_reg = Dhat_2 + regularizer\n",
    "\n",
    "    # Solve the OpInf problem by solving the regularized normal equations.\n",
    "    Ohat = np.linalg.solve(Dhat_2_reg, np.dot(Dhat.T, Qhat_2)).T\n",
    "\n",
    "    # Extract the linear, quadratic, and constant reduced model operators.\n",
    "    Ahat = Ohat[:, :r]\n",
    "    Fhat = Ohat[:, r : r + s]\n",
    "    chat = Ohat[:, r + s]\n",
    "\n",
    "    end_time_OpInf_learning = time.time()\n",
    "\n",
    "    # Define the OpInf reduced model.\n",
    "    OpInf_red_model = lambda x: Ahat @ x + Fhat @ compute_Qhat_sq(x) + chat\n",
    "\n",
    "    # Extract the reduced initial condition from Qhat_1.\n",
    "    qhat0 = Qhat_1[0, :]\n",
    "\n",
    "    # Compute the reduced solution over the trial time horizon, which here is the same as the target time horizon\n",
    "    start_time_OpInf_eval = time.time()\n",
    "    contains_nans, Qtilde_OpInf = solve_opinf_difference_model(\n",
    "        initial_condition=qhat0,\n",
    "        n_steps=nt_p,\n",
    "        reduced_model=OpInf_red_model,\n",
    "    )\n",
    "    end_time_OpInf_eval = time.time()\n",
    "\n",
    "    time_OpInf_eval = end_time_OpInf_eval - start_time_OpInf_eval\n",
    "\n",
    "    # If the model produced an unstable solution, move on to the next\n",
    "    # regularization candidates.\n",
    "    if contains_nans:\n",
    "        continue  # go to next iteration of the for loop.\n",
    "\n",
    "    # If the ratio of the maximum coefficient grown exceeds the allowed\n",
    "    # threshold, move on to the next regularization candidates.\n",
    "    max_diff_Qhat_trial = np.max(\n",
    "        np.abs(Qtilde_OpInf - mean_Qhat_train), axis=0\n",
    "    )\n",
    "    max_growth_trial = np.max(max_diff_Qhat_trial) / np.max(\n",
    "        max_diff_Qhat_train\n",
    "    )\n",
    "    if max_growth_trial > max_growth:\n",
    "        continue\n",
    "\n",
    "    # At this point we know the model produced a stable solution without too\n",
    "    # much growth. Compute the training error and, if it better than the\n",
    "    # current best error, save the regularization, reduced solution, and\n",
    "    # the learning times.\n",
    "    train_err = compute_train_err(Qhat_global.T[:nt, :], Qtilde_OpInf[:nt, :])\n",
    "    if train_err < best_train_err:\n",
    "        best_beta1 = beta1\n",
    "        best_beta2 = beta2\n",
    "        best_train_err = train_err\n",
    "        Qtilde_OpInf_opt = Qtilde_OpInf\n",
    "        OpInf_ROM_wtime_opt = time_OpInf_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c70840-b0f1-4dfe-afc8-e03f431e78d3",
   "metadata": {},
   "source": [
    "### Step 5: Postprocessing of the reduced solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2c8741",
   "metadata": {},
   "source": [
    "In the final step, we postprocess the obtained reduced solution. Specifically, we use the approximate velocity solutions to compute the vorticity \n",
    "$$\n",
    "\\begin{equation} \n",
    "    \\omega = \\frac{\\partial u_y}{\\partial x} - \\frac{\\partial u_x}{\\partial y}\n",
    "\\end{equation}\n",
    "$$\n",
    "at three probe locations positioned near the mid-channel, with increasing distance from the circular cylinder, namely $(0.40, 0.20), (0.60, 0.20)$, and $(1.00, 0.20)$. The corresponding indices within each snapshot for these locations are $\\{997; 1,469; 1,376\\}$. \n",
    "\n",
    "Since estimating the vorticity requires FEniCS functionality, we saved the reference and reduced model solutions here [in this folder](./navier_stokes_benchmark/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5713863c-7e4d-4342-b3fd-82c09fdc4879",
   "metadata": {},
   "source": [
    "### We start by plotting the singular value decay and corresponding retained energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36ec34c-5c74-476e-8edb-4a0d12b96d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "no_kept_svals_global = 200\n",
    "no_kept_svals_energy = 20\n",
    "no_svals_global = range(1, no_kept_svals_global + 1)\n",
    "no_svals_energy = range(1, no_kept_svals_energy + 1)\n",
    "\n",
    "retained_energy = np.cumsum(eigs) / np.sum(eigs)\n",
    "target_ret_energy = 0.999\n",
    "\n",
    "r = np.argmax(retained_energy > target_ret_energy) + 1\n",
    "ret_energy = retained_energy[r]\n",
    "\n",
    "plt.rcParams[\"lines.linewidth\"] = 0\n",
    "plt.rc(\"figure\", dpi=400)\n",
    "plt.rc(\"font\", family=\"serif\")\n",
    "plt.rc(\"legend\", edgecolor=\"none\")\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 2)\n",
    "plt.rcParams.update({\"font.size\": 5})\n",
    "\n",
    "charcoal = [0.1, 0.1, 0.1]\n",
    "color1 = \"#D55E00\"\n",
    "color2 = \"#0072B2\"\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "plt.rc(\"figure\", facecolor=\"w\")\n",
    "plt.rc(\"axes\", facecolor=\"w\", edgecolor=\"k\", labelcolor=\"k\")\n",
    "plt.rc(\"savefig\", facecolor=\"w\")\n",
    "plt.rc(\"text\", color=\"k\")\n",
    "plt.rc(\"xtick\", color=\"k\")\n",
    "plt.rc(\"ytick\", color=\"k\")\n",
    "\n",
    "ax1.spines[\"right\"].set_visible(False)\n",
    "ax1.spines[\"top\"].set_visible(False)\n",
    "ax1.yaxis.set_ticks_position(\"left\")\n",
    "ax1.xaxis.set_ticks_position(\"bottom\")\n",
    "\n",
    "ax2.spines[\"right\"].set_visible(False)\n",
    "ax2.spines[\"top\"].set_visible(False)\n",
    "ax2.yaxis.set_ticks_position(\"left\")\n",
    "ax2.xaxis.set_ticks_position(\"bottom\")\n",
    "\n",
    "## plot\n",
    "ax1.semilogy(\n",
    "    no_svals_global,\n",
    "    np.sqrt(eigs)[:no_kept_svals_global] / np.sqrt(eigs[0]),\n",
    "    linestyle=\"-\",\n",
    "    lw=0.75,\n",
    "    color=color1,\n",
    ")\n",
    "ax1.set_xlabel(\"index\")\n",
    "ax1.set_ylabel(\"singular values transformed data\")\n",
    "\n",
    "ax2.plot(\n",
    "    no_svals_energy,\n",
    "    retained_energy[:no_kept_svals_energy],\n",
    "    linestyle=\"-\",\n",
    "    lw=0.75,\n",
    "    color=color1,\n",
    ")\n",
    "ax2.set_xlabel(\"reduced dimension\")\n",
    "ax2.set_ylabel(\"% energy retained\")\n",
    "ax2.plot(\n",
    "    [r, r], [0, retained_energy[r]], linestyle=\"--\", lw=0.5, color=charcoal\n",
    ")\n",
    "ax2.plot(\n",
    "    [0, r],\n",
    "    [retained_energy[r], retained_energy[r]],\n",
    "    linestyle=\"--\",\n",
    "    lw=0.5,\n",
    "    color=charcoal,\n",
    ")\n",
    "\n",
    "xlim = ax1.get_xlim()\n",
    "ax1.set_ylim([1e-10, 1.02e0])\n",
    "\n",
    "x_pos_all = np.array([0, 99, 199])\n",
    "labels = np.array([1, 100, 200])\n",
    "ax1.set_xticks(x_pos_all)\n",
    "ax1.set_xticklabels(labels)\n",
    "\n",
    "ax2.set_xlim([0, 20])\n",
    "ax2.set_ylim([0.4, 1.001])\n",
    "\n",
    "ax2.set_xticks([1, r, 20])\n",
    "ax2.set_yticks([0.5, 0.75, ret_energy, 1])\n",
    "ax2.set_yticklabels([r\"$50\\%$\", r\"$75\\%$\", r\"$99.93\\%$\", \"\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4355729f-1b73-4046-8f47-fa1ddaae2c10",
   "metadata": {},
   "source": [
    "### Ploting the ROM approximate vorticity solutions at the three probe locations, $(0.40, 0.20), (0.60, 0.20)$, and $(1.00, 0.20)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505d372f",
   "metadata": {},
   "source": [
    "The hashed areas on the left mark the training time horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2140ea-a520-4cb0-9554-2f9744a9d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = 4\n",
    "t_end = 10\n",
    "t_train = 7\n",
    "dt = 1e-2\n",
    "\n",
    "t = np.arange(t_start, t_end, dt)\n",
    "\n",
    "ref_data = np.load(\"./ref_vorticity_full_time_domain.npy\")\n",
    "\n",
    "ref_data_loc1 = ref_data[400:, 0]\n",
    "ref_data_loc2 = ref_data[400:, 1]\n",
    "ref_data_loc3 = ref_data[400:, 2]\n",
    "\n",
    "OpInf_data = np.load(\"./ROM_vorticity_probes.npy\")\n",
    "\n",
    "OpInf_data_loc1 = OpInf_data[:, 0]\n",
    "OpInf_data_loc2 = OpInf_data[:, 1]\n",
    "OpInf_data_loc3 = OpInf_data[:, 2]\n",
    "\n",
    "\n",
    "plt.rc(\"figure\", dpi=400)\n",
    "plt.rc(\"font\", family=\"serif\")\n",
    "plt.rc(\"legend\", edgecolor=\"none\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 2)\n",
    "plt.rcParams.update({\"font.size\": 6})\n",
    "\n",
    "charcoal = [0.1, 0.1, 0.1]\n",
    "color1 = \"#D55E00\"\n",
    "color2 = \"#0072B2\"\n",
    "\n",
    "fig = plt.figure()\n",
    "ax11 = fig.add_subplot(131)\n",
    "ax12 = fig.add_subplot(132, sharex=ax11)\n",
    "ax13 = fig.add_subplot(133, sharex=ax11)\n",
    "\n",
    "plt.rc(\"figure\", facecolor=\"w\")\n",
    "plt.rc(\"axes\", facecolor=\"w\", edgecolor=\"k\", labelcolor=\"k\")\n",
    "plt.rc(\"savefig\", facecolor=\"w\")\n",
    "plt.rc(\"text\", color=\"k\")\n",
    "plt.rc(\"xtick\", color=\"k\")\n",
    "plt.rc(\"ytick\", color=\"k\")\n",
    "\n",
    "\n",
    "ax11.spines[\"right\"].set_visible(False)\n",
    "ax11.spines[\"top\"].set_visible(False)\n",
    "ax11.yaxis.set_ticks_position(\"left\")\n",
    "ax11.xaxis.set_ticks_position(\"bottom\")\n",
    "\n",
    "ax12.spines[\"right\"].set_visible(False)\n",
    "ax12.spines[\"top\"].set_visible(False)\n",
    "ax12.yaxis.set_ticks_position(\"left\")\n",
    "ax12.xaxis.set_ticks_position(\"bottom\")\n",
    "\n",
    "\n",
    "ax13.spines[\"right\"].set_visible(False)\n",
    "ax13.spines[\"top\"].set_visible(False)\n",
    "ax13.yaxis.set_ticks_position(\"left\")\n",
    "ax13.xaxis.set_ticks_position(\"bottom\")\n",
    "\n",
    "ax11.plot(t, ref_data_loc1, linestyle=\"-\", lw=1.00, color=charcoal)\n",
    "ax11.plot(t, OpInf_data_loc1, linestyle=\"--\", lw=1.00, color=color1)\n",
    "\n",
    "ax12.plot(t, ref_data_loc2, linestyle=\"-\", lw=1.00, color=charcoal)\n",
    "ax12.plot(t, OpInf_data_loc2, linestyle=\"--\", lw=1.00, color=color1)\n",
    "\n",
    "ax13.plot(t, ref_data_loc3, linestyle=\"-\", lw=1.00, color=charcoal)\n",
    "ax13.plot(t, OpInf_data_loc3, linestyle=\"--\", lw=1.00, color=color1)\n",
    "\n",
    "ax11.axvline(x=t_train, lw=1.00, linestyle=\"--\", color=\"gray\")\n",
    "ax12.axvline(x=t_train, lw=1.00, linestyle=\"--\", color=\"gray\")\n",
    "ax13.axvline(x=t_train, lw=1.00, linestyle=\"--\", color=\"gray\")\n",
    "\n",
    "ax11.set_title(\"probe 1 (0.4, 0.2)\")\n",
    "ax12.set_title(\"probe 2 (0.6, 0.2)\")\n",
    "ax13.set_title(\"probe 3 (1.0, 0.2)\")\n",
    "\n",
    "ax11.set_ylabel(r\"$\\omega$\")\n",
    "\n",
    "fig.supxlabel(\"target time horizon (seconds)\", y=-0.1)\n",
    "\n",
    "xlim = ax11.get_xlim()\n",
    "ax11.set_xlim([xlim[0], 10])\n",
    "\n",
    "ylim1 = ax11.get_ylim()\n",
    "ylim2 = ax12.get_ylim()\n",
    "ylim3 = ax13.get_ylim()\n",
    "\n",
    "h1 = np.abs(ylim1[0]) + np.abs(ylim1[1])\n",
    "h2 = np.abs(ylim2[0]) + np.abs(ylim2[1])\n",
    "h3 = np.abs(ylim3[0]) + np.abs(ylim3[1])\n",
    "\n",
    "rect = plt.Rectangle(\n",
    "    (0, ylim1[0]),\n",
    "    width=t_train,\n",
    "    height=h1,\n",
    "    hatch=\"/\",\n",
    "    color=\"grey\",\n",
    "    alpha=0.2,\n",
    "    label=\"training region\",\n",
    ")\n",
    "ax11.add_patch(rect)\n",
    "rect = plt.Rectangle(\n",
    "    (0, ylim2[0]),\n",
    "    width=t_train,\n",
    "    height=h2,\n",
    "    hatch=\"/\",\n",
    "    color=\"grey\",\n",
    "    alpha=0.2,\n",
    "    label=\"training region\",\n",
    ")\n",
    "ax12.add_patch(rect)\n",
    "rect = plt.Rectangle(\n",
    "    (0, ylim3[0]),\n",
    "    width=t_train,\n",
    "    height=h3,\n",
    "    hatch=\"/\",\n",
    "    color=\"grey\",\n",
    "    alpha=0.2,\n",
    "    label=\"training region\",\n",
    ")\n",
    "ax13.add_patch(rect)\n",
    "\n",
    "x_pos_all = np.array([4, 5, 6, 7, 8, 9, 10])\n",
    "labels = np.array([4, 5, 6, 7, 8, 9, 10])\n",
    "ax11.set_xticks(x_pos_all)\n",
    "ax11.set_xticklabels(labels)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
